{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047b5594-e916-42cb-857c-011eb56502fc",
   "metadata": {},
   "source": [
    "***\n",
    "# <center> ***Forecasting Performance Measures***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8223a1c2-1126-48d4-82b5-338226516bde",
   "metadata": {},
   "source": [
    "Time series prediction performance measures provide a summary of the skill and capability of the forecast model that made the predictions. There are many different **performance measures** to choose from. It can be confusing to know which measure to use and how to interpret the results. **Time series** generally focus on the prediction of real values, called **regression problems**. Therefore the performance measures will focus on methods for evaluating real-valued predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfcdd7c-982c-4bcf-a8d0-89c36b52f0df",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Forecast Error (or Residual Forecast Error)***\n",
    "***\n",
    "\n",
    "The **forecast error** is calculated as the expected value minus the predicted value. This is called the **residual error** of the prediction.\n",
    "\n",
    "**<center>forecast error = expected value - predicted value</center>**\n",
    "The forecast error can be calculated for each prediction, providing a time series of forecast errors. The example below demonstrates how the forecast error can be calculated for a series of 5 predictions compared to 5 expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a650d78e-d1ba-40ed-bd9a-d0756813d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast Errors: [-0.2, 0.09999999999999998, -0.1, -0.09999999999999998, -0.2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "expected = [0.0, 0.5, 0.0, 0.5, 0.0]\n",
    "predictions = [0.2, 0.4, 0.1, 0.6, 0.2]\n",
    "forecast_errors = [expected[i]-predictions[i] for i in range(len(expected))]\n",
    "print(f\"Forecast Errors: {forecast_errors}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0926dc67-bbd6-4098-b7d7-e9d416d2d070",
   "metadata": {},
   "source": [
    "***The units of the forecast error are the same as the units of the prediction. A forecast error of zero indicates no error, or perfect skill for that forecast.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4320221-6188-45d0-8dfa-7aa3cd38fe63",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Mean Forecast Error (or Forecast Bias)***\n",
    "***\n",
    "\n",
    "Mean forecast error is calculated as the average of the forecast error values.\n",
    "**<center>mean forecast error = mean(forecast error)</center>**\n",
    "Forecast errors can be positive and negative. **This means that when the average of these values is calculated, an ideal mean forecast error would be zero**. A mean forecast error value other than zero suggests a tendency of the model to over forecast (negative error) or under forecast (positive error). As such, the mean forecast error is also called the forecast bias. The forecast error can be calculated directly as the mean of the forecast values. The example below demonstrates how the mean of the forecast errors can be calculated manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecf3abad-5807-4cec-b3e0-a6a16af779da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: -0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "forecast_errors = [expected[i]-predictions[i] for i in range(len(expected))]\n",
    "bias = sum(forecast_errors) * 1.0 / len(expected)\n",
    "print(f'Bias: {bias}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0268ff33-0f2b-49fe-a159-ec0fa7b91970",
   "metadata": {},
   "source": [
    "***The units of the forecast bias are the same as the units of the predictions. A forecast bias of zero, or a very small number near zero, shows an unbiased model.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe4de0-846e-4a8f-b6ab-9f4a0c2e2770",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Mean Absolute Error***\n",
    "***\n",
    "\n",
    "**The mean absolute error**, or MAE, is calculated as the average of the forecast error values, where all of the forecast values are forced to be positive. Forcing values to be positive is called making them absolute. This is signified by the absolute function `abs()`\n",
    "\n",
    "**<center>mean absolute error = mean(abs(forecast error))</center>**\n",
    "Where **abs()** makes values positive, forecast and **mean()** calculates the average value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d91e8de5-903d-4ef8-ae7c-846fc9c0630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.14000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "expected = [0.0, 0.5, 0.0, 0.5, 0.0]\n",
    "predictions = [0.2, 0.4, 0.1, 0.6, 0.2]\n",
    "mae = mean_absolute_error(expected, predictions)\n",
    "print(f\"MAE: {mae:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6486cf16-c0db-4546-b311-4dec8c2468dd",
   "metadata": {},
   "source": [
    "***These error values are in the original units of the predicted values. A mean absolute error of zero indicates no error.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485225d-384b-4ab2-83dc-880331eaeec0",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Mean Squared Error***\n",
    "***\n",
    "\n",
    "**The mean squared error**, or MSE, is calculated as the average of the squared forecast error values. Squaring the forecast error values forces them to be positive; it also has the e ect of putting more weight on large errors. Very large or outlier forecast errors are squared, which in turn has the effect of dragging the mean of the squared forecast errors out resulting in a larger\n",
    "mean squared error score. In effect, the score gives worse performance to those models that make large wrong forecasts.\n",
    "\n",
    "**<center>mean squared error = mean(forecast error^2)</center>**\n",
    "\n",
    "We can use the mean squared error() function from **scikit-learn** to calculate the mean squared error for a list of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e856b8a-c864-4286-a80b-00cab6311fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:0.02200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "expected = [0.0, 0.5, 0.0, 0.5, 0.0]\n",
    "predictions = [0.2, 0.4, 0.1, 0.6, 0.2]\n",
    "mse = mean_squared_error(expected, predictions)\n",
    "print(f'MSE:{mse:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71206ba0-e962-417e-bdc3-df7c076513a3",
   "metadata": {},
   "source": [
    "***The error values are in squared units of the predicted values. A mean squared error of zero indicates perfect skill, or no error.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb691a-baa1-4b5c-94ae-80d415f72981",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Root Mean Squared Error***\n",
    "***\n",
    "\n",
    "**The mean squared error** described above is in the squared units of the predictions. It can be transformed back into the original units of the predictions by taking the square root of the mean squared error score. This is called the root mean squared error, or RMSE.\n",
    "\n",
    "$$\n",
    "rmse = \\sqrt{mean squared error}\n",
    "$$\n",
    "\n",
    "This can be calculated by using the **sqrt()** math function on the mean squared error calculated using the **mean squared error() scikit-learn** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ae746c6-45c1-40c7-a625-5aaf9bc962f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:0.14832\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "mse = mean_squared_error(expected, predictions)\n",
    "rmse = sqrt(mse)\n",
    "print(f'RMSE:{rmse:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca52f00-8010-4415-b7d0-dd739d55979e",
   "metadata": {},
   "source": [
    "**The RMES error values are in the same units as the predictions. As with the mean squared error, an RMSE of zero indicates no error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ed4ff5d-ed4f-4d8b-9ee3-abebc4dce8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb7faa9f-17d2-4b39-bb39-716a028e1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_num = np.random.randint(100,1000,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19a17732-fd61-44ea-af3d-89d36a853176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11786"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(row_num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "127e2707-785a-4d02-8704-3783f5ed691a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589.3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(row_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e005483-8377-409c-b1d6-3b93f8a3ca5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589.3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "count = 0\n",
    "for i in row_num:\n",
    "    sum += i\n",
    "    count += 1\n",
    "\n",
    "sum / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b20cba7-8f33-459c-b448-d93340399a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[425,\n",
       " 760,\n",
       " 570,\n",
       " 673,\n",
       " 968,\n",
       " 979,\n",
       " 889,\n",
       " 889,\n",
       " 690,\n",
       " 934,\n",
       " 839,\n",
       " 772,\n",
       " 488,\n",
       " 322,\n",
       " 653,\n",
       " 995,\n",
       " 538,\n",
       " 999,\n",
       " 617,\n",
       " 547]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(row_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b8668-53b7-41e8-9637-27a0f6ea7025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
